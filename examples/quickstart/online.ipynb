{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quickstart Example of Off-Policy Evaluation of Online Bandit Algorithms\n",
    "---\n",
    "This notebook provides an example of conducting OPE of online bandit algorithms using Replay Method (RM) with synthetic bandit feedback data.\n",
    "RM uses a subset of the logged bandit feedback data where actions selected by the behavior policy are the same as that of the evaluation policy.\n",
    "Theoretically, RM is unbiased when the behavior policy is uniformly random and the evaluation policy is fixed.\n",
    "However, empirically, RM works well when evaluation policies are learning algorithms.\n",
    "Please refer to https://arxiv.org/abs/1003.5956 about the details of RM.\n",
    "\n",
    "Our example with online bandit algorithms contains the follwoing three major steps:\n",
    "- (1) Synthetic Data Generation\n",
    "- (2) Off-Policy Evaluation (OPE)\n",
    "- (3) Evaluation of OPE\n",
    "\n",
    "Please see [../examples/online](../online) for a more sophisticated example of the evaluation of OPE of online bandit algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# import open bandit pipeline (obp)\n",
    "import obp\n",
    "from obp.dataset import (\n",
    "    SyntheticBanditDataset,\n",
    "    logistic_reward_function\n",
    ")\n",
    "from obp.policy import EpsilonGreedy, LinTS, LinUCB\n",
    "from obp.ope import (\n",
    "    OffPolicyEvaluation, \n",
    "    ReplayMethod\n",
    ")\n",
    "from obp.simulator import run_bandit_simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3.3\n"
     ]
    }
   ],
   "source": [
    "# obp version\n",
    "print(obp.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1) Synthetic Data Generation\n",
    "We prepare easy-to-use synthetic data generator: `SyntheticBanditDataset` class in the dataset module.\n",
    "\n",
    "It takes number of actions (`n_actions`), dimension of context vectors (`dim_context`), reward function (`reward_function`), and behavior policy (`behavior_policy_function`) as inputs and generates a synthetic bandit dataset that can be used to evaluate the performance of decision making policies (obtained by `off-policy learning`) and OPE estimators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_rounds': 100000,\n",
       " 'n_actions': 10,\n",
       " 'context': array([[-0.20470766,  0.47894334, -0.51943872, -0.5557303 ,  1.96578057],\n",
       "        [ 1.39340583,  0.09290788,  0.28174615,  0.76902257,  1.24643474],\n",
       "        [ 1.00718936, -1.29622111,  0.27499163,  0.22891288,  1.35291684],\n",
       "        ...,\n",
       "        [ 1.36946256,  0.58727761, -0.69296769, -0.27519988, -2.10289159],\n",
       "        [-0.27428715,  0.52635353,  1.02572168, -0.18486381,  0.72464834],\n",
       "        [-1.25579833, -1.42455203, -0.26361242,  0.27928604,  1.21015571]]),\n",
       " 'action_context': array([[1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1]]),\n",
       " 'action': array([8, 4, 0, ..., 8, 9, 7]),\n",
       " 'position': array([0, 0, 0, ..., 0, 0, 0]),\n",
       " 'reward': array([0, 1, 1, ..., 1, 1, 0]),\n",
       " 'expected_reward': array([[0.80210203, 0.73828559, 0.83199558, ..., 0.81190503, 0.70617705,\n",
       "         0.68985306],\n",
       "        [0.94119582, 0.93473317, 0.91345213, ..., 0.94140688, 0.93152449,\n",
       "         0.90132868],\n",
       "        [0.87248862, 0.67974991, 0.66965669, ..., 0.79229752, 0.82712978,\n",
       "         0.74923536],\n",
       "        ...,\n",
       "        [0.64856003, 0.38145901, 0.84476094, ..., 0.40962057, 0.77114661,\n",
       "         0.65752798],\n",
       "        [0.73208527, 0.82012699, 0.78161352, ..., 0.72361416, 0.8652249 ,\n",
       "         0.82571751],\n",
       "        [0.40348366, 0.24485417, 0.24037926, ..., 0.49613133, 0.30714854,\n",
       "         0.5527749 ]]),\n",
       " 'pscore': array([0.1, 0.1, 0.1, ..., 0.1, 0.1, 0.1])}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate a synthetic bandit dataset with 10 actions\n",
    "# we use `logistic function` as the reward function\n",
    "# we use the uniformly random behavior policy because it is desriable for RM\n",
    "# one can define their own reward function and behavior policy such as nonlinear ones. \n",
    "dataset = SyntheticBanditDataset(\n",
    "    n_actions=10,\n",
    "    dim_context=5,\n",
    "    reward_type=\"binary\", # \"binary\" or \"continuous\"\n",
    "    reward_function=logistic_reward_function,\n",
    "    behavior_policy_function=None, # uniformly random\n",
    "    random_state=12345,\n",
    ")\n",
    "# obtain a set of synthetic logged bandit feedback\n",
    "n_rounds = 100000\n",
    "bandit_feedback = dataset.obtain_batch_bandit_feedback(n_rounds=n_rounds)\n",
    "\n",
    "# `bandit_feedback` is a dictionary storing synthetic logged bandit feedback\n",
    "bandit_feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2) Off-Policy Evaluation (OPE)\n",
    "Our next step is OPE which attempts to estimate the performance of online bandit algorithms using the logged bandit feedback and RM.\n",
    "\n",
    "Here, we show the OPE results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [00:01<00:00, 86658.89it/s]\n",
      "100%|██████████| 100000/100000 [02:16<00:00, 734.95it/s]\n",
      "100%|██████████| 100000/100000 [00:15<00:00, 6392.74it/s]\n"
     ]
    }
   ],
   "source": [
    "# simulations of online bandit algorithms\n",
    "# obtain a deterministic action distribution representing which action is selected at each round in the simulation\n",
    "# policies are updated only when the selected action is the same as that of the logged data\n",
    "evaluation_policy_a = EpsilonGreedy(\n",
    "    n_actions=dataset.n_actions,\n",
    "    epsilon=0.1,\n",
    "    random_state=12345\n",
    ")\n",
    "action_dist_a = run_bandit_simulation(bandit_feedback, evaluation_policy_a)\n",
    "\n",
    "evaluation_policy_b = LinTS(\n",
    "    dim=dataset.dim_context,\n",
    "    n_actions=dataset.n_actions,\n",
    "    random_state=12345\n",
    ")\n",
    "action_dist_b = run_bandit_simulation(bandit_feedback, evaluation_policy_b)\n",
    "\n",
    "evaluation_policy_c = LinUCB(\n",
    "    dim=dataset.dim_context,\n",
    "    n_actions=dataset.n_actions,\n",
    "    random_state=12345\n",
    ")\n",
    "action_dist_c = run_bandit_simulation(bandit_feedback, evaluation_policy_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# estimate the policy value of the online bandit algorithms using RM\n",
    "ope = OffPolicyEvaluation(\n",
    "    bandit_feedback=bandit_feedback,\n",
    "    ope_estimators=[ReplayMethod()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`estimated_rewards_by_reg_model` is not given; model dependent estimators such as DM or DR cannot be used.\n",
      "`estimated_rewards_by_reg_model` is not given; model dependent estimators such as DM or DR cannot be used.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    95.0% CI (lower)  95.0% CI (upper)      mean\n",
      "rm          0.601181          0.632591  0.615177 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# estimate the policy value of EpsilonGreedy\n",
    "estimated_policy_value_a, estimated_interval_a = ope.summarize_off_policy_estimates(\n",
    "    action_dist=action_dist_a\n",
    ")\n",
    "print(estimated_interval_a, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`estimated_rewards_by_reg_model` is not given; model dependent estimators such as DM or DR cannot be used.\n",
      "`estimated_rewards_by_reg_model` is not given; model dependent estimators such as DM or DR cannot be used.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    95.0% CI (lower)  95.0% CI (upper)      mean\n",
      "rm           0.69415          0.725614  0.709859 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# estimate the policy value of LinTS\n",
    "estimated_policy_value_b, estimated_interval_b = ope.summarize_off_policy_estimates(\n",
    "    action_dist=action_dist_b\n",
    ")\n",
    "print(estimated_interval_b, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`estimated_rewards_by_reg_model` is not given; model dependent estimators such as DM or DR cannot be used.\n",
      "`estimated_rewards_by_reg_model` is not given; model dependent estimators such as DM or DR cannot be used.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    95.0% CI (lower)  95.0% CI (upper)      mean\n",
      "rm           0.66958          0.697205  0.682631 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# estimate the policy value of LinUCB\n",
    "estimated_policy_value_c, estimated_interval_c = ope.summarize_off_policy_estimates(\n",
    "    action_dist=action_dist_c\n",
    ")\n",
    "print(estimated_interval_c, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RM estimates that LinTS is the best policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (3) Evaluation of OPE\n",
    "Our final step is **the evaluation of OPE**, which evaluates and compares the estimation accuracy of OPE estimators.\n",
    "\n",
    "With synthetic data, we can calculate the policy value of the evaluation policies. \n",
    "Therefore, we can compare the policy values estimated by RM with the ground-turths to evaluate the accuracy of OPE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy value of EpsilonGreedy: 0.6104368686610999\n",
      "policy value of LinTS: 0.7182133343802966\n",
      "policy value of LinUCB: 0.681387402045725\n"
     ]
    }
   ],
   "source": [
    "# we first calculate the policy values of the three evaluation policies using the expected rewards of the test data\n",
    "expected_rewards = bandit_feedback['expected_reward']\n",
    "policy_value_a = np.average(expected_rewards, weights=action_dist_a[:, :, 0], axis=1).mean()\n",
    "policy_value_b = np.average(expected_rewards, weights=action_dist_b[:, :, 0], axis=1).mean()\n",
    "policy_value_c = np.average(expected_rewards, weights=action_dist_c[:, :, 0], axis=1).mean()\n",
    "\n",
    "print(f'policy value of EpsilonGreedy: {policy_value_a}')\n",
    "print(f'policy value of LinTS: {policy_value_b}')\n",
    "print(f'policy value of LinUCB: {policy_value_c}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact, LinTS reveals the best performance among the three evaluation policies.\n",
    "\n",
    "Using the above policy values, we evaluate the estimation accuracy of the OPE estimators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`estimated_rewards_by_reg_model` is not given; model dependent estimators such as DM or DR cannot be used.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>relative-ee</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>rm</th>\n",
       "      <td>0.008859</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    relative-ee\n",
       "rm     0.008859"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate the estimation performances of OPE estimators \n",
    "# by comparing the estimated policy values of EpsilonGreedy and its ground-truth.\n",
    "# `summarize_estimators_comparison` returns a pandas dataframe containing estimation performances of given estimators \n",
    "relative_ee_a = ope.summarize_estimators_comparison(\n",
    "    ground_truth_policy_value=policy_value_a,\n",
    "    action_dist=action_dist_a,\n",
    "    metric=\"relative-ee\", # \"relative-ee\" (relative estimation error) or \"se\" (squared error)\n",
    ")\n",
    "\n",
    "# estimation performances of the three estimators (lower means accurate)\n",
    "relative_ee_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`estimated_rewards_by_reg_model` is not given; model dependent estimators such as DM or DR cannot be used.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>relative-ee</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>rm</th>\n",
       "      <td>0.009187</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    relative-ee\n",
       "rm     0.009187"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate the estimation performance of OPE estimators \n",
    "# by comparing the estimated policy values of LinTS t and its ground-truth.\n",
    "# `summarize_estimators_comparison` returns a pandas dataframe containing estimation performances of given estimators \n",
    "relative_ee_b = ope.summarize_estimators_comparison(\n",
    "    ground_truth_policy_value=policy_value_b,\n",
    "    action_dist=action_dist_b,\n",
    "    metric=\"relative-ee\", # \"relative-ee\" (relative estimation error) or \"se\" (squared error)\n",
    ")\n",
    "\n",
    "# estimation performances of the three estimators (lower means accurate)\n",
    "relative_ee_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`estimated_rewards_by_reg_model` is not given; model dependent estimators such as DM or DR cannot be used.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>relative-ee</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>rm</th>\n",
       "      <td>0.001211</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    relative-ee\n",
       "rm     0.001211"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate the estimation performance of OPE estimators \n",
    "# by comparing the estimated policy values of LinUCB and its ground-truth.\n",
    "# `summarize_estimators_comparison` returns a pandas dataframe containing estimation performances of given estimators \n",
    "relative_ee_c = ope.summarize_estimators_comparison(\n",
    "    ground_truth_policy_value=policy_value_c,\n",
    "    action_dist=action_dist_c,\n",
    "    metric=\"relative-ee\", # \"relative-ee\" (relative estimation error) or \"se\" (squared error)\n",
    ")\n",
    "\n",
    "# estimation performances of the three estimators (lower means accurate)\n",
    "relative_ee_c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please see [../examples/online](../online) for a more sophisticated example of the evaluation of OPE with online bandit algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
