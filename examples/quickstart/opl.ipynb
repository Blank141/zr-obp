{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Quickstart Example with Off-Policy Learners\n",
    "---\n",
    "This notebook provides an example of implementing several off-policy learning methods with synthetic logged bandit data.\n",
    "\n",
    "The example consists of the follwoing four major steps:\n",
    "- (1) Generating Synthetic Data\n",
    "- (2) Off-Policy Learning\n",
    "- (3) Evaluation of Off-Policy Learners\n",
    "\n",
    "Please see [../examples/opl](../opl) for a more sophisticated example of the evaluation of off-policy learners with synthetic bandit data."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "# needed on Colab\n",
    "# !pip install obp"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier as RandomForest\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# import open bandit pipeline (obp)\n",
    "import obp\n",
    "from obp.dataset import (\n",
    "    SyntheticBanditDataset,\n",
    "    logistic_reward_function,\n",
    "    linear_reward_function,\n",
    "    linear_behavior_policy\n",
    ")\n",
    "from obp.policy import (\n",
    "    IPWLearner, \n",
    "    NNPolicyLearner, \n",
    "    Random\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# obp version\n",
    "print(obp.__version__)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.4.1\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## (1) Generating Synthetic Data\n",
    "`obp.dataset.SyntheticBanditDataset` is an easy-to-use synthetic data generator.\n",
    "\n",
    "It takes \n",
    "- number of actions (`n_actions`, $|\\mathcal{A}|$)\n",
    "- dimension of context vectors (`dim_context`, $d$)\n",
    "- reward function (`reward_function`, $q(x,a)=\\mathbb{E}[r \\mid x,a]$)\n",
    "- behavior policy (`behavior_policy_function`, $\\pi_b(a|x)$) \n",
    "\n",
    "as inputs and generates a synthetic logged bandit data that can be used to evaluate the performance of decision making policies (obtained by `off-policy learning`) and OPE estimators."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "# generate a synthetic bandit dataset with 10 actions\n",
    "# we use `logistic function` as the reward function and `linear_behavior_policy` as the behavior policy.\n",
    "# one can define their own reward function and behavior policy such as nonlinear ones. \n",
    "dataset = SyntheticBanditDataset(\n",
    "    n_actions=10,\n",
    "    dim_context=5,\n",
    "    tau=0.2,\n",
    "    reward_type=\"binary\", # \"binary\" or \"continuous\"\n",
    "    reward_function=logistic_reward_function,\n",
    "    behavior_policy_function=linear_behavior_policy,\n",
    "    random_state=12345,\n",
    ")"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "# obtain training and test sets of synthetic logged bandit data\n",
    "n_rounds_train, n_rounds_test = 1000, 10000\n",
    "bandit_feedback_train = dataset.obtain_batch_bandit_feedback(n_rounds=n_rounds_train)\n",
    "bandit_feedback_test = dataset.obtain_batch_bandit_feedback(n_rounds=n_rounds_test)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "the logged bandit data is collected by the behavior policy as follows.\n",
    "\n",
    "$ \\mathcal{D}_b := \\{(x_i,a_i,r_i)\\}_{i=1}^n$  where $(x,a,r) \\sim p(x)\\pi_b(a \\mid x)p(r \\mid x,a) $"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "# `bandit_feedback` is a dictionary storing synthetic logged bandit feedback\n",
    "bandit_feedback_train"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'n_rounds': 1000,\n",
       " 'n_actions': 10,\n",
       " 'context': array([[-0.20470766,  0.47894334, -0.51943872, -0.5557303 ,  1.96578057],\n",
       "        [ 1.39340583,  0.09290788,  0.28174615,  0.76902257,  1.24643474],\n",
       "        [ 1.00718936, -1.29622111,  0.27499163,  0.22891288,  1.35291684],\n",
       "        ...,\n",
       "        [ 0.68519813, -0.69338139, -0.48637144, -1.36880376, -0.76710644],\n",
       "        [-0.00526111,  0.30822176, -0.52216256,  0.17505628, -1.02944391],\n",
       "        [ 2.23819622, -0.92851973, -0.58784203,  1.42936092, -2.17340945]]),\n",
       " 'action_context': array([[1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1]]),\n",
       " 'action': array([6, 1, 1, 1, 4, 5, 6, 5, 6, 5, 6, 6, 0, 1, 1, 5, 6, 6, 6, 6, 5, 6,\n",
       "        3, 1, 3, 6, 9, 5, 6, 1, 0, 6, 6, 0, 3, 4, 5, 1, 6, 6, 6, 3, 6, 1,\n",
       "        1, 1, 3, 3, 6, 1, 4, 1, 6, 6, 5, 1, 3, 2, 6, 6, 2, 1, 4, 1, 1, 6,\n",
       "        6, 4, 6, 5, 6, 3, 2, 6, 3, 1, 1, 6, 3, 6, 6, 4, 1, 6, 2, 3, 1, 2,\n",
       "        5, 1, 6, 6, 6, 6, 1, 6, 1, 6, 6, 0, 1, 1, 5, 1, 1, 3, 6, 6, 5, 4,\n",
       "        6, 1, 6, 3, 0, 6, 5, 3, 5, 3, 6, 3, 1, 4, 5, 6, 3, 1, 1, 3, 3, 0,\n",
       "        6, 6, 6, 2, 0, 6, 6, 1, 1, 5, 2, 1, 1, 6, 1, 5, 3, 2, 6, 6, 1, 1,\n",
       "        1, 6, 6, 6, 1, 4, 3, 0, 2, 2, 5, 5, 6, 6, 2, 6, 6, 1, 9, 6, 6, 2,\n",
       "        6, 6, 6, 0, 0, 1, 9, 1, 1, 6, 1, 6, 3, 0, 6, 6, 1, 0, 6, 1, 1, 3,\n",
       "        1, 4, 6, 2, 3, 6, 2, 0, 4, 3, 6, 2, 6, 3, 4, 5, 3, 5, 1, 6, 2, 1,\n",
       "        1, 1, 1, 6, 1, 1, 5, 2, 5, 5, 1, 1, 1, 1, 0, 0, 1, 1, 5, 6, 6, 6,\n",
       "        1, 2, 1, 6, 4, 6, 1, 6, 1, 6, 6, 6, 1, 1, 3, 2, 3, 1, 5, 6, 6, 2,\n",
       "        6, 3, 6, 1, 3, 6, 6, 1, 0, 6, 1, 3, 6, 6, 6, 1, 6, 5, 4, 6, 6, 1,\n",
       "        1, 2, 0, 5, 5, 2, 2, 3, 4, 1, 9, 3, 3, 1, 1, 6, 1, 6, 1, 6, 1, 6,\n",
       "        6, 1, 2, 5, 6, 1, 1, 2, 6, 0, 1, 5, 1, 1, 5, 1, 6, 1, 5, 3, 1, 1,\n",
       "        6, 1, 6, 1, 1, 6, 3, 2, 0, 5, 1, 1, 1, 1, 1, 0, 0, 1, 1, 4, 1, 9,\n",
       "        1, 5, 4, 3, 5, 1, 6, 1, 5, 1, 0, 5, 5, 5, 6, 1, 2, 5, 6, 1, 1, 1,\n",
       "        6, 6, 3, 4, 1, 1, 6, 6, 6, 2, 6, 8, 1, 6, 1, 6, 4, 4, 6, 3, 3, 5,\n",
       "        5, 1, 2, 5, 6, 1, 1, 2, 1, 4, 4, 3, 1, 1, 1, 5, 6, 6, 6, 5, 1, 6,\n",
       "        6, 2, 6, 4, 6, 6, 1, 6, 1, 3, 1, 3, 5, 6, 5, 6, 6, 1, 1, 1, 0, 6,\n",
       "        6, 1, 5, 6, 1, 6, 0, 1, 6, 3, 2, 1, 3, 6, 3, 3, 4, 6, 1, 5, 5, 0,\n",
       "        5, 0, 5, 3, 2, 6, 1, 1, 3, 5, 6, 0, 3, 5, 6, 1, 3, 3, 6, 2, 5, 0,\n",
       "        6, 2, 6, 1, 5, 6, 3, 7, 5, 6, 6, 6, 6, 9, 1, 6, 1, 1, 3, 4, 1, 1,\n",
       "        1, 1, 5, 1, 2, 1, 1, 1, 8, 3, 6, 4, 5, 6, 2, 2, 6, 6, 6, 1, 2, 3,\n",
       "        6, 4, 6, 1, 4, 4, 6, 2, 6, 1, 1, 6, 6, 5, 1, 1, 1, 2, 6, 6, 4, 1,\n",
       "        2, 1, 1, 6, 6, 1, 6, 2, 0, 6, 2, 1, 1, 5, 1, 6, 1, 6, 4, 1, 6, 1,\n",
       "        6, 6, 0, 5, 5, 1, 6, 1, 3, 1, 4, 2, 5, 6, 6, 2, 1, 3, 4, 1, 0, 5,\n",
       "        6, 1, 6, 1, 6, 1, 3, 6, 6, 1, 4, 5, 5, 5, 1, 1, 4, 1, 1, 4, 5, 0,\n",
       "        8, 1, 2, 5, 1, 1, 5, 6, 4, 1, 6, 6, 6, 6, 6, 6, 3, 3, 5, 6, 2, 5,\n",
       "        5, 4, 1, 2, 1, 6, 6, 1, 6, 6, 3, 6, 3, 1, 2, 1, 1, 1, 6, 6, 5, 6,\n",
       "        1, 5, 1, 6, 5, 3, 6, 1, 0, 1, 5, 3, 6, 1, 6, 6, 6, 1, 6, 5, 1, 9,\n",
       "        4, 5, 6, 4, 4, 1, 1, 2, 1, 6, 1, 9, 6, 6, 6, 0, 2, 0, 1, 5, 8, 5,\n",
       "        3, 0, 6, 6, 3, 1, 1, 5, 9, 5, 6, 1, 6, 4, 7, 6, 1, 1, 1, 1, 5, 1,\n",
       "        4, 6, 1, 1, 2, 6, 5, 6, 1, 1, 1, 6, 6, 1, 5, 3, 1, 6, 5, 1, 1, 1,\n",
       "        6, 3, 1, 1, 1, 6, 1, 1, 5, 6, 0, 1, 6, 1, 6, 3, 3, 6, 5, 1, 0, 5,\n",
       "        6, 3, 6, 2, 2, 6, 6, 6, 1, 1, 1, 6, 0, 1, 3, 1, 6, 6, 6, 1, 6, 6,\n",
       "        4, 1, 1, 5, 6, 3, 5, 3, 2, 5, 6, 3, 1, 3, 3, 3, 2, 6, 1, 0, 6, 6,\n",
       "        1, 5, 1, 1, 3, 5, 1, 3, 6, 3, 3, 5, 4, 1, 6, 1, 1, 1, 2, 1, 2, 6,\n",
       "        2, 6, 6, 6, 1, 4, 1, 6, 1, 6, 1, 5, 6, 0, 1, 0, 4, 1, 5, 0, 1, 6,\n",
       "        6, 3, 3, 0, 6, 6, 6, 6, 3, 0, 1, 6, 6, 1, 1, 6, 2, 5, 5, 1, 5, 3,\n",
       "        6, 1, 1, 3, 3, 6, 1, 9, 1, 6, 6, 6, 6, 1, 0, 4, 6, 1, 6, 0, 1, 4,\n",
       "        3, 6, 1, 6, 1, 3, 1, 6, 6, 1, 4, 6, 3, 1, 6, 1, 5, 1, 6, 4, 1, 6,\n",
       "        3, 5, 9, 6, 6, 6, 1, 1, 1, 2, 3, 6, 6, 7, 2, 6, 5, 3, 1, 1, 3, 1,\n",
       "        4, 4, 0, 1, 1, 6, 1, 6, 6, 4, 6, 1, 6, 5, 6, 1, 1, 6, 2, 5, 6, 6,\n",
       "        1, 3, 1, 1, 6, 1, 1, 3, 6, 4, 1, 2, 1, 4, 6, 6, 6, 1, 1, 1, 6, 1,\n",
       "        2, 1, 2, 6, 6, 0, 4, 6, 2, 1]),\n",
       " 'position': None,\n",
       " 'reward': array([0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1,\n",
       "        0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1,\n",
       "        0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1,\n",
       "        0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,\n",
       "        1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1,\n",
       "        1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1,\n",
       "        1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0,\n",
       "        0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1,\n",
       "        1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0,\n",
       "        1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0,\n",
       "        1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1,\n",
       "        0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0,\n",
       "        0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1,\n",
       "        1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0,\n",
       "        0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0,\n",
       "        1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1,\n",
       "        0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1,\n",
       "        0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0,\n",
       "        1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1,\n",
       "        0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0,\n",
       "        1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0,\n",
       "        0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1,\n",
       "        1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,\n",
       "        0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1,\n",
       "        0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0,\n",
       "        0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1,\n",
       "        0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,\n",
       "        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1,\n",
       "        0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0,\n",
       "        0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0,\n",
       "        0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1,\n",
       "        0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0,\n",
       "        1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1,\n",
       "        1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n",
       "        0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0,\n",
       "        1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1,\n",
       "        0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0,\n",
       "        0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,\n",
       "        0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1,\n",
       "        0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1,\n",
       "        0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1,\n",
       "        0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0,\n",
       "        0, 1, 0, 1, 1, 0, 1, 0, 1, 0]),\n",
       " 'expected_reward': array([[0.80210203, 0.73828559, 0.83199558, ..., 0.81190503, 0.70617705,\n",
       "         0.68985306],\n",
       "        [0.94119582, 0.93473317, 0.91345213, ..., 0.94140688, 0.93152449,\n",
       "         0.90132868],\n",
       "        [0.87248862, 0.67974991, 0.66965669, ..., 0.79229752, 0.82712978,\n",
       "         0.74923536],\n",
       "        ...,\n",
       "        [0.53596053, 0.12596475, 0.55864771, ..., 0.18471776, 0.59124514,\n",
       "         0.36183789],\n",
       "        [0.49485165, 0.39630524, 0.6830731 , ..., 0.48456511, 0.5906923 ,\n",
       "         0.68027971],\n",
       "        [0.78052501, 0.46782835, 0.7403455 , ..., 0.66094588, 0.80231157,\n",
       "         0.80418207]]),\n",
       " 'pscore': array([0.29815101, 0.30297159, 0.30297159, 0.30297159, 0.06400209,\n",
       "        0.10247721, 0.29815101, 0.10247721, 0.29815101, 0.10247721,\n",
       "        0.29815101, 0.29815101, 0.04788441, 0.30297159, 0.30297159,\n",
       "        0.10247721, 0.29815101, 0.29815101, 0.29815101, 0.29815101,\n",
       "        0.10247721, 0.29815101, 0.10308971, 0.30297159, 0.10308971,\n",
       "        0.29815101, 0.01085456, 0.10247721, 0.29815101, 0.30297159,\n",
       "        0.04788441, 0.29815101, 0.29815101, 0.04788441, 0.10308971,\n",
       "        0.06400209, 0.10247721, 0.30297159, 0.29815101, 0.29815101,\n",
       "        0.29815101, 0.10308971, 0.29815101, 0.30297159, 0.30297159,\n",
       "        0.30297159, 0.10308971, 0.10308971, 0.29815101, 0.30297159,\n",
       "        0.06400209, 0.30297159, 0.29815101, 0.29815101, 0.10247721,\n",
       "        0.30297159, 0.10308971, 0.06387652, 0.29815101, 0.29815101,\n",
       "        0.06387652, 0.30297159, 0.06400209, 0.30297159, 0.30297159,\n",
       "        0.29815101, 0.29815101, 0.06400209, 0.29815101, 0.10247721,\n",
       "        0.29815101, 0.10308971, 0.06387652, 0.29815101, 0.10308971,\n",
       "        0.30297159, 0.30297159, 0.29815101, 0.10308971, 0.29815101,\n",
       "        0.29815101, 0.06400209, 0.30297159, 0.29815101, 0.06387652,\n",
       "        0.10308971, 0.30297159, 0.06387652, 0.10247721, 0.30297159,\n",
       "        0.29815101, 0.29815101, 0.29815101, 0.29815101, 0.30297159,\n",
       "        0.29815101, 0.30297159, 0.29815101, 0.29815101, 0.04788441,\n",
       "        0.30297159, 0.30297159, 0.10247721, 0.30297159, 0.30297159,\n",
       "        0.10308971, 0.29815101, 0.29815101, 0.10247721, 0.06400209,\n",
       "        0.29815101, 0.30297159, 0.29815101, 0.10308971, 0.04788441,\n",
       "        0.29815101, 0.10247721, 0.10308971, 0.10247721, 0.10308971,\n",
       "        0.29815101, 0.10308971, 0.30297159, 0.06400209, 0.10247721,\n",
       "        0.29815101, 0.10308971, 0.30297159, 0.30297159, 0.10308971,\n",
       "        0.10308971, 0.04788441, 0.29815101, 0.29815101, 0.29815101,\n",
       "        0.06387652, 0.04788441, 0.29815101, 0.29815101, 0.30297159,\n",
       "        0.30297159, 0.10247721, 0.06387652, 0.30297159, 0.30297159,\n",
       "        0.29815101, 0.30297159, 0.10247721, 0.10308971, 0.06387652,\n",
       "        0.29815101, 0.29815101, 0.30297159, 0.30297159, 0.30297159,\n",
       "        0.29815101, 0.29815101, 0.29815101, 0.30297159, 0.06400209,\n",
       "        0.10308971, 0.04788441, 0.06387652, 0.06387652, 0.10247721,\n",
       "        0.10247721, 0.29815101, 0.29815101, 0.06387652, 0.29815101,\n",
       "        0.29815101, 0.30297159, 0.01085456, 0.29815101, 0.29815101,\n",
       "        0.06387652, 0.29815101, 0.29815101, 0.29815101, 0.04788441,\n",
       "        0.04788441, 0.30297159, 0.01085456, 0.30297159, 0.30297159,\n",
       "        0.29815101, 0.30297159, 0.29815101, 0.10308971, 0.04788441,\n",
       "        0.29815101, 0.29815101, 0.30297159, 0.04788441, 0.29815101,\n",
       "        0.30297159, 0.30297159, 0.10308971, 0.30297159, 0.06400209,\n",
       "        0.29815101, 0.06387652, 0.10308971, 0.29815101, 0.06387652,\n",
       "        0.04788441, 0.06400209, 0.10308971, 0.29815101, 0.06387652,\n",
       "        0.29815101, 0.10308971, 0.06400209, 0.10247721, 0.10308971,\n",
       "        0.10247721, 0.30297159, 0.29815101, 0.06387652, 0.30297159,\n",
       "        0.30297159, 0.30297159, 0.30297159, 0.29815101, 0.30297159,\n",
       "        0.30297159, 0.10247721, 0.06387652, 0.10247721, 0.10247721,\n",
       "        0.30297159, 0.30297159, 0.30297159, 0.30297159, 0.04788441,\n",
       "        0.04788441, 0.30297159, 0.30297159, 0.10247721, 0.29815101,\n",
       "        0.29815101, 0.29815101, 0.30297159, 0.06387652, 0.30297159,\n",
       "        0.29815101, 0.06400209, 0.29815101, 0.30297159, 0.29815101,\n",
       "        0.30297159, 0.29815101, 0.29815101, 0.29815101, 0.30297159,\n",
       "        0.30297159, 0.10308971, 0.06387652, 0.10308971, 0.30297159,\n",
       "        0.10247721, 0.29815101, 0.29815101, 0.06387652, 0.29815101,\n",
       "        0.10308971, 0.29815101, 0.30297159, 0.10308971, 0.29815101,\n",
       "        0.29815101, 0.30297159, 0.04788441, 0.29815101, 0.30297159,\n",
       "        0.10308971, 0.29815101, 0.29815101, 0.29815101, 0.30297159,\n",
       "        0.29815101, 0.10247721, 0.06400209, 0.29815101, 0.29815101,\n",
       "        0.30297159, 0.30297159, 0.06387652, 0.04788441, 0.10247721,\n",
       "        0.10247721, 0.06387652, 0.06387652, 0.10308971, 0.06400209,\n",
       "        0.30297159, 0.01085456, 0.10308971, 0.10308971, 0.30297159,\n",
       "        0.30297159, 0.29815101, 0.30297159, 0.29815101, 0.30297159,\n",
       "        0.29815101, 0.30297159, 0.29815101, 0.29815101, 0.30297159,\n",
       "        0.06387652, 0.10247721, 0.29815101, 0.30297159, 0.30297159,\n",
       "        0.06387652, 0.29815101, 0.04788441, 0.30297159, 0.10247721,\n",
       "        0.30297159, 0.30297159, 0.10247721, 0.30297159, 0.29815101,\n",
       "        0.30297159, 0.10247721, 0.10308971, 0.30297159, 0.30297159,\n",
       "        0.29815101, 0.30297159, 0.29815101, 0.30297159, 0.30297159,\n",
       "        0.29815101, 0.10308971, 0.06387652, 0.04788441, 0.10247721,\n",
       "        0.30297159, 0.30297159, 0.30297159, 0.30297159, 0.30297159,\n",
       "        0.04788441, 0.04788441, 0.30297159, 0.30297159, 0.06400209,\n",
       "        0.30297159, 0.01085456, 0.30297159, 0.10247721, 0.06400209,\n",
       "        0.10308971, 0.10247721, 0.30297159, 0.29815101, 0.30297159,\n",
       "        0.10247721, 0.30297159, 0.04788441, 0.10247721, 0.10247721,\n",
       "        0.10247721, 0.29815101, 0.30297159, 0.06387652, 0.10247721,\n",
       "        0.29815101, 0.30297159, 0.30297159, 0.30297159, 0.29815101,\n",
       "        0.29815101, 0.10308971, 0.06400209, 0.30297159, 0.30297159,\n",
       "        0.29815101, 0.29815101, 0.29815101, 0.06387652, 0.29815101,\n",
       "        0.00415075, 0.30297159, 0.29815101, 0.30297159, 0.29815101,\n",
       "        0.06400209, 0.06400209, 0.29815101, 0.10308971, 0.10308971,\n",
       "        0.10247721, 0.10247721, 0.30297159, 0.06387652, 0.10247721,\n",
       "        0.29815101, 0.30297159, 0.30297159, 0.06387652, 0.30297159,\n",
       "        0.06400209, 0.06400209, 0.10308971, 0.30297159, 0.30297159,\n",
       "        0.30297159, 0.10247721, 0.29815101, 0.29815101, 0.29815101,\n",
       "        0.10247721, 0.30297159, 0.29815101, 0.29815101, 0.06387652,\n",
       "        0.29815101, 0.06400209, 0.29815101, 0.29815101, 0.30297159,\n",
       "        0.29815101, 0.30297159, 0.10308971, 0.30297159, 0.10308971,\n",
       "        0.10247721, 0.29815101, 0.10247721, 0.29815101, 0.29815101,\n",
       "        0.30297159, 0.30297159, 0.30297159, 0.04788441, 0.29815101,\n",
       "        0.29815101, 0.30297159, 0.10247721, 0.29815101, 0.30297159,\n",
       "        0.29815101, 0.04788441, 0.30297159, 0.29815101, 0.10308971,\n",
       "        0.06387652, 0.30297159, 0.10308971, 0.29815101, 0.10308971,\n",
       "        0.10308971, 0.06400209, 0.29815101, 0.30297159, 0.10247721,\n",
       "        0.10247721, 0.04788441, 0.10247721, 0.04788441, 0.10247721,\n",
       "        0.10308971, 0.06387652, 0.29815101, 0.30297159, 0.30297159,\n",
       "        0.10308971, 0.10247721, 0.29815101, 0.04788441, 0.10308971,\n",
       "        0.10247721, 0.29815101, 0.30297159, 0.10308971, 0.10308971,\n",
       "        0.29815101, 0.06387652, 0.10247721, 0.04788441, 0.29815101,\n",
       "        0.06387652, 0.29815101, 0.30297159, 0.10247721, 0.29815101,\n",
       "        0.10308971, 0.00254214, 0.10247721, 0.29815101, 0.29815101,\n",
       "        0.29815101, 0.29815101, 0.01085456, 0.30297159, 0.29815101,\n",
       "        0.30297159, 0.30297159, 0.10308971, 0.06400209, 0.30297159,\n",
       "        0.30297159, 0.30297159, 0.30297159, 0.10247721, 0.30297159,\n",
       "        0.06387652, 0.30297159, 0.30297159, 0.30297159, 0.00415075,\n",
       "        0.10308971, 0.29815101, 0.06400209, 0.10247721, 0.29815101,\n",
       "        0.06387652, 0.06387652, 0.29815101, 0.29815101, 0.29815101,\n",
       "        0.30297159, 0.06387652, 0.10308971, 0.29815101, 0.06400209,\n",
       "        0.29815101, 0.30297159, 0.06400209, 0.06400209, 0.29815101,\n",
       "        0.06387652, 0.29815101, 0.30297159, 0.30297159, 0.29815101,\n",
       "        0.29815101, 0.10247721, 0.30297159, 0.30297159, 0.30297159,\n",
       "        0.06387652, 0.29815101, 0.29815101, 0.06400209, 0.30297159,\n",
       "        0.06387652, 0.30297159, 0.30297159, 0.29815101, 0.29815101,\n",
       "        0.30297159, 0.29815101, 0.06387652, 0.04788441, 0.29815101,\n",
       "        0.06387652, 0.30297159, 0.30297159, 0.10247721, 0.30297159,\n",
       "        0.29815101, 0.30297159, 0.29815101, 0.06400209, 0.30297159,\n",
       "        0.29815101, 0.30297159, 0.29815101, 0.29815101, 0.04788441,\n",
       "        0.10247721, 0.10247721, 0.30297159, 0.29815101, 0.30297159,\n",
       "        0.10308971, 0.30297159, 0.06400209, 0.06387652, 0.10247721,\n",
       "        0.29815101, 0.29815101, 0.06387652, 0.30297159, 0.10308971,\n",
       "        0.06400209, 0.30297159, 0.04788441, 0.10247721, 0.29815101,\n",
       "        0.30297159, 0.29815101, 0.30297159, 0.29815101, 0.30297159,\n",
       "        0.10308971, 0.29815101, 0.29815101, 0.30297159, 0.06400209,\n",
       "        0.10247721, 0.10247721, 0.10247721, 0.30297159, 0.30297159,\n",
       "        0.06400209, 0.30297159, 0.30297159, 0.06400209, 0.10247721,\n",
       "        0.04788441, 0.00415075, 0.30297159, 0.06387652, 0.10247721,\n",
       "        0.30297159, 0.30297159, 0.10247721, 0.29815101, 0.06400209,\n",
       "        0.30297159, 0.29815101, 0.29815101, 0.29815101, 0.29815101,\n",
       "        0.29815101, 0.29815101, 0.10308971, 0.10308971, 0.10247721,\n",
       "        0.29815101, 0.06387652, 0.10247721, 0.10247721, 0.06400209,\n",
       "        0.30297159, 0.06387652, 0.30297159, 0.29815101, 0.29815101,\n",
       "        0.30297159, 0.29815101, 0.29815101, 0.10308971, 0.29815101,\n",
       "        0.10308971, 0.30297159, 0.06387652, 0.30297159, 0.30297159,\n",
       "        0.30297159, 0.29815101, 0.29815101, 0.10247721, 0.29815101,\n",
       "        0.30297159, 0.10247721, 0.30297159, 0.29815101, 0.10247721,\n",
       "        0.10308971, 0.29815101, 0.30297159, 0.04788441, 0.30297159,\n",
       "        0.10247721, 0.10308971, 0.29815101, 0.30297159, 0.29815101,\n",
       "        0.29815101, 0.29815101, 0.30297159, 0.29815101, 0.10247721,\n",
       "        0.30297159, 0.01085456, 0.06400209, 0.10247721, 0.29815101,\n",
       "        0.06400209, 0.06400209, 0.30297159, 0.30297159, 0.06387652,\n",
       "        0.30297159, 0.29815101, 0.30297159, 0.01085456, 0.29815101,\n",
       "        0.29815101, 0.29815101, 0.04788441, 0.06387652, 0.04788441,\n",
       "        0.30297159, 0.10247721, 0.00415075, 0.10247721, 0.10308971,\n",
       "        0.04788441, 0.29815101, 0.29815101, 0.10308971, 0.30297159,\n",
       "        0.30297159, 0.10247721, 0.01085456, 0.10247721, 0.29815101,\n",
       "        0.30297159, 0.29815101, 0.06400209, 0.00254214, 0.29815101,\n",
       "        0.30297159, 0.30297159, 0.30297159, 0.30297159, 0.10247721,\n",
       "        0.30297159, 0.06400209, 0.29815101, 0.30297159, 0.30297159,\n",
       "        0.06387652, 0.29815101, 0.10247721, 0.29815101, 0.30297159,\n",
       "        0.30297159, 0.30297159, 0.29815101, 0.29815101, 0.30297159,\n",
       "        0.10247721, 0.10308971, 0.30297159, 0.29815101, 0.10247721,\n",
       "        0.30297159, 0.30297159, 0.30297159, 0.29815101, 0.10308971,\n",
       "        0.30297159, 0.30297159, 0.30297159, 0.29815101, 0.30297159,\n",
       "        0.30297159, 0.10247721, 0.29815101, 0.04788441, 0.30297159,\n",
       "        0.29815101, 0.30297159, 0.29815101, 0.10308971, 0.10308971,\n",
       "        0.29815101, 0.10247721, 0.30297159, 0.04788441, 0.10247721,\n",
       "        0.29815101, 0.10308971, 0.29815101, 0.06387652, 0.06387652,\n",
       "        0.29815101, 0.29815101, 0.29815101, 0.30297159, 0.30297159,\n",
       "        0.30297159, 0.29815101, 0.04788441, 0.30297159, 0.10308971,\n",
       "        0.30297159, 0.29815101, 0.29815101, 0.29815101, 0.30297159,\n",
       "        0.29815101, 0.29815101, 0.06400209, 0.30297159, 0.30297159,\n",
       "        0.10247721, 0.29815101, 0.10308971, 0.10247721, 0.10308971,\n",
       "        0.06387652, 0.10247721, 0.29815101, 0.10308971, 0.30297159,\n",
       "        0.10308971, 0.10308971, 0.10308971, 0.06387652, 0.29815101,\n",
       "        0.30297159, 0.04788441, 0.29815101, 0.29815101, 0.30297159,\n",
       "        0.10247721, 0.30297159, 0.30297159, 0.10308971, 0.10247721,\n",
       "        0.30297159, 0.10308971, 0.29815101, 0.10308971, 0.10308971,\n",
       "        0.10247721, 0.06400209, 0.30297159, 0.29815101, 0.30297159,\n",
       "        0.30297159, 0.30297159, 0.06387652, 0.30297159, 0.06387652,\n",
       "        0.29815101, 0.06387652, 0.29815101, 0.29815101, 0.29815101,\n",
       "        0.30297159, 0.06400209, 0.30297159, 0.29815101, 0.30297159,\n",
       "        0.29815101, 0.30297159, 0.10247721, 0.29815101, 0.04788441,\n",
       "        0.30297159, 0.04788441, 0.06400209, 0.30297159, 0.10247721,\n",
       "        0.04788441, 0.30297159, 0.29815101, 0.29815101, 0.10308971,\n",
       "        0.10308971, 0.04788441, 0.29815101, 0.29815101, 0.29815101,\n",
       "        0.29815101, 0.10308971, 0.04788441, 0.30297159, 0.29815101,\n",
       "        0.29815101, 0.30297159, 0.30297159, 0.29815101, 0.06387652,\n",
       "        0.10247721, 0.10247721, 0.30297159, 0.10247721, 0.10308971,\n",
       "        0.29815101, 0.30297159, 0.30297159, 0.10308971, 0.10308971,\n",
       "        0.29815101, 0.30297159, 0.01085456, 0.30297159, 0.29815101,\n",
       "        0.29815101, 0.29815101, 0.29815101, 0.30297159, 0.04788441,\n",
       "        0.06400209, 0.29815101, 0.30297159, 0.29815101, 0.04788441,\n",
       "        0.30297159, 0.06400209, 0.10308971, 0.29815101, 0.30297159,\n",
       "        0.29815101, 0.30297159, 0.10308971, 0.30297159, 0.29815101,\n",
       "        0.29815101, 0.30297159, 0.06400209, 0.29815101, 0.10308971,\n",
       "        0.30297159, 0.29815101, 0.30297159, 0.10247721, 0.30297159,\n",
       "        0.29815101, 0.06400209, 0.30297159, 0.29815101, 0.10308971,\n",
       "        0.10247721, 0.01085456, 0.29815101, 0.29815101, 0.29815101,\n",
       "        0.30297159, 0.30297159, 0.30297159, 0.06387652, 0.10308971,\n",
       "        0.29815101, 0.29815101, 0.00254214, 0.06387652, 0.29815101,\n",
       "        0.10247721, 0.10308971, 0.30297159, 0.30297159, 0.10308971,\n",
       "        0.30297159, 0.06400209, 0.06400209, 0.04788441, 0.30297159,\n",
       "        0.30297159, 0.29815101, 0.30297159, 0.29815101, 0.29815101,\n",
       "        0.06400209, 0.29815101, 0.30297159, 0.29815101, 0.10247721,\n",
       "        0.29815101, 0.30297159, 0.30297159, 0.29815101, 0.06387652,\n",
       "        0.10247721, 0.29815101, 0.29815101, 0.30297159, 0.10308971,\n",
       "        0.30297159, 0.30297159, 0.29815101, 0.30297159, 0.30297159,\n",
       "        0.10308971, 0.29815101, 0.06400209, 0.30297159, 0.06387652,\n",
       "        0.30297159, 0.06400209, 0.29815101, 0.29815101, 0.29815101,\n",
       "        0.30297159, 0.30297159, 0.30297159, 0.29815101, 0.30297159,\n",
       "        0.06387652, 0.30297159, 0.06387652, 0.29815101, 0.29815101,\n",
       "        0.04788441, 0.06400209, 0.29815101, 0.06387652, 0.30297159])}"
      ]
     },
     "metadata": {},
     "execution_count": 32
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## (2) Off-Policy Learning\n",
    "After generating synthetic data, we now train some decision making policies.\n",
    "\n",
    "To train policies, we use\n",
    "\n",
    "- `obp.policy.NNPolicyLearner` (Neural Network Policy Learner)\n",
    "- `obp.policy.IPWLearner`\n",
    "\n",
    "For NN Learner, we use `obp.ope.DirectMethod`, `obp.ope.InverseProbabilityWeighting`, and `obp.ope.DoublyRobust` as its objective functions.\n",
    "For IPW Learner, we use *RandomForestClassifier* and *LogisticRegression* implemented in scikit-learn for base machine learning methods."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "A policy is trained by maximizing an OPE estimator as an objective function as follows.\n",
    "\n",
    "$$ \\hat{\\pi} \\in \\arg \\max_{\\pi \\in \\Pi} \\hat{V} (\\pi; \\mathcal{D}_{tr}) - \\lambda \\cdot \\Omega (\\pi)  $$\n",
    "\n",
    "where $\\mathcal{D}_{tr}$ is a training bandit dataset. $\\Omega (\\cdot)$ is a regularization term."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "# define NNPolicyLearner with DM as its objective function\n",
    "nn_dm = NNPolicyLearner(\n",
    "    n_actions=dataset.n_actions,\n",
    "    dim_context=dataset.dim_context,\n",
    "    off_policy_objective=\"dm\",\n",
    "    random_state=12345,\n",
    ")\n",
    "\n",
    "# train NNPolicyLearner on the training set of logged bandit data\n",
    "nn_dm.fit(\n",
    "    context=bandit_feedback_train[\"context\"],\n",
    "    action=bandit_feedback_train[\"action\"],\n",
    "    reward=bandit_feedback_train[\"reward\"],\n",
    ")\n",
    "\n",
    "# obtains action choice probabilities for the test set of logged bandit data\n",
    "action_dist_nn_dm = nn_dm.predict_proba(\n",
    "    context=bandit_feedback_test[\"context\"]\n",
    ")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "q-func learning:   0%|          | 0/100 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (200x33 and 34x100)",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/p_/_yy0rm4521b_p15mxwsvd0fh0000gn/T/ipykernel_3965/3853760251.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# train NNPolicyLearner on the training set of logged bandit data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m nn_dm.fit(\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mcontext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbandit_feedback_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"context\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0maction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbandit_feedback_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"action\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.9.5/envs/zr-obp/lib/python3.9/site-packages/obp-0.4.1-py3.9.egg/obp/policy/offline.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, context, action, reward, pscore, position)\u001b[0m\n\u001b[1;32m    819\u001b[0m         \u001b[0;31m# train q function estimator when it is needed to train NNPolicy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moff_policy_objective\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"ipw\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 821\u001b[0;31m             self.q_func_estimator.fit(\n\u001b[0m\u001b[1;32m    822\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    823\u001b[0m                 \u001b[0maction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.9.5/envs/zr-obp/lib/python3.9/site-packages/obp-0.4.1-py3.9.egg/obp/policy/offline.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, context, action, reward)\u001b[0m\n\u001b[1;32m   1540\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtraining_data_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1541\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1542\u001b[0;31m                 \u001b[0mq_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1543\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq_hat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1544\u001b[0m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.9.5/envs/zr-obp/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.9.5/envs/zr-obp/lib/python3.9/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.9.5/envs/zr-obp/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.9.5/envs/zr-obp/lib/python3.9/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.9.5/envs/zr-obp/lib/python3.9/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1845\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1846\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1847\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1848\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (200x33 and 34x100)"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "# define NNPolicyLearner with IPW as its objective function\n",
    "nn_ipw = NNPolicyLearner(\n",
    "    n_actions=dataset.n_actions,\n",
    "    dim_context=dataset.dim_context,\n",
    "    off_policy_objective=\"ipw\",\n",
    "    random_state=12345,\n",
    ")\n",
    "\n",
    "# train NNPolicyLearner on the training set of logged bandit data\n",
    "nn_ipw.fit(\n",
    "    context=bandit_feedback_train[\"context\"],\n",
    "    action=bandit_feedback_train[\"action\"],\n",
    "    reward=bandit_feedback_train[\"reward\"],\n",
    "    pscore=bandit_feedback_train[\"pscore\"],\n",
    ")\n",
    "\n",
    "# obtains action choice probabilities for the test set of logged bandit data\n",
    "action_dist_nn_ipw = nn_ipw.predict_proba(\n",
    "    context=bandit_feedback_test[\"context\"]\n",
    ")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "policy learning: 100%|██████████| 1000/1000 [00:25<00:00, 38.66it/s]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "# define NNPolicyLearner with DR as its objective function\n",
    "nn_dr = NNPolicyLearner(\n",
    "    n_actions=dataset.n_actions,\n",
    "    dim_context=dataset.dim_context,\n",
    "    off_policy_objective=\"dr\",\n",
    "    random_state=12345,\n",
    ")\n",
    "\n",
    "# train NNPolicyLearner on the training set of logged bandit data\n",
    "nn_dr.fit(\n",
    "    context=bandit_feedback_train[\"context\"],\n",
    "    action=bandit_feedback_train[\"action\"],\n",
    "    reward=bandit_feedback_train[\"reward\"],\n",
    "    pscore=bandit_feedback_train[\"pscore\"],\n",
    ")\n",
    "\n",
    "# obtains action choice probabilities for the test set of logged bandit data\n",
    "action_dist_nn_dr = nn_dr.predict_proba(\n",
    "    context=bandit_feedback_test[\"context\"]\n",
    ")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "q-func learning: 100%|██████████| 100/100 [00:00<00:00, 304.38it/s]\n",
      "policy learning: 100%|██████████| 100/100 [00:05<00:00, 17.32it/s]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "# define IPWLearner with Logistic Regression as its base ML model\n",
    "ipw_lr = IPWLearner(\n",
    "    n_actions=dataset.n_actions,\n",
    "    base_classifier=LogisticRegression(C=100, random_state=12345)\n",
    ")\n",
    "\n",
    "# train IPWLearner on the training set of logged bandit data\n",
    "ipw_lr.fit(\n",
    "    context=bandit_feedback_train[\"context\"],\n",
    "    action=bandit_feedback_train[\"action\"],\n",
    "    reward=bandit_feedback_train[\"reward\"],\n",
    "    pscore=bandit_feedback_train[\"pscore\"]\n",
    ")\n",
    "\n",
    "# obtains action choice probabilities for the test set of logged bandit data\n",
    "action_dist_ipw_lr = ipw_lr.predict(\n",
    "    context=bandit_feedback_test[\"context\"]\n",
    ")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/Users/usaito/.pyenv/versions/3.9.5/envs/zr-obp/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "# define IPWLearner with Random Forest as its base ML model\n",
    "ipw_rf = IPWLearner(\n",
    "    n_actions=dataset.n_actions,\n",
    "    base_classifier=RandomForest(\n",
    "        n_estimators=30, min_samples_leaf=10, random_state=12345\n",
    "    )\n",
    ")\n",
    "\n",
    "# train IPWLearner on the training set of logged bandit data\n",
    "ipw_rf.fit(\n",
    "    context=bandit_feedback_train[\"context\"],\n",
    "    action=bandit_feedback_train[\"action\"],\n",
    "    reward=bandit_feedback_train[\"reward\"],\n",
    "    pscore=bandit_feedback_train[\"pscore\"]\n",
    ")\n",
    "\n",
    "# obtains action choice probabilities for the test set of logged bandit data\n",
    "action_dist_ipw_rf = ipw_rf.predict(\n",
    "    context=bandit_feedback_test[\"context\"]\n",
    ")"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "# define Uniform Random Policy as a baseline evaluation policy\n",
    "random = Random(n_actions=dataset.n_actions,)\n",
    "\n",
    "# compute the action choice probabililties for the test set of logged bandit data\n",
    "action_dist_random = random.compute_batch_action_dist(\n",
    "    n_rounds=bandit_feedback_test[\"n_rounds\"]\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "# action_dist is a probability distribution over actions (can be deterministic)\n",
    "action_dist_ipw_lr[:, :, 0]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 1., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## (3) Evaluation of Off-Policy Learners\n",
    "Our final step is the evaluation and comparison of the off-policy learnres.\n",
    "\n",
    "With synthetic data, we can calculate the policy value of the off-policy learners as follows. \n",
    "\n",
    "$$V(\\pi_e) \\approx \\frac{1}{|\\mathcal{D}_{te}|} \\sum_{i=1}^{|\\mathcal{D}_{te}|} \\mathbb{E}_{a \\sim \\pi_e(a|x_i)} [q(x_i, a)], \\; \\, where \\; \\, q(x,a) := \\mathbb{E}_{r \\sim p(r|x,a)} [r]$$\n",
    "\n",
    "where $\\mathcal{D}_{te}$ is the test set of logged bandit data."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "# we calculate the policy values of the trained policies based on the expected rewards of the test data\n",
    "policy_names = [\n",
    "    \"NN Policy Learner with DM\",\n",
    "    \"NN Policy Learner with IPW\",\n",
    "    \"NN Policy Learner with DR\",\n",
    "    \"IPW Learner with Logistic Regression\",\n",
    "    \"IPW Learner with Random Forest\",\n",
    "    \"Unifrom Random\"\n",
    "]\n",
    "action_dist_list = [\n",
    "    action_dist_nn_dm,\n",
    "    action_dist_nn_ipw,\n",
    "    action_dist_nn_dr,\n",
    "    action_dist_ipw_lr,\n",
    "    action_dist_ipw_rf,\n",
    "    action_dist_random\n",
    "]\n",
    "for name, action_dist in zip(policy_names, action_dist_list):\n",
    "    true_policy_value = dataset.calc_ground_truth_policy_value(\n",
    "        expected_reward=bandit_feedback_test[\"expected_reward\"],\n",
    "        action_dist=action_dist,\n",
    "    )\n",
    "    print(f'policy value of {name}: {true_policy_value}')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "policy value of NN Policy Learner with DM: 0.6447528278341246\n",
      "policy value of NN Policy Learner with IPW: 0.6529194200649526\n",
      "policy value of NN Policy Learner with DR: 0.6086503953895724\n",
      "policy value of IPW Learner with Logistic Regression: 0.6386706155005949\n",
      "policy value of IPW Learner with Random Forest: 0.6378215876656457\n",
      "policy value of Unifrom Random: 0.6031317016319008\n"
     ]
    }
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In fact, IPW Learner with Logistic Regression is the best, and NN Policy Learner with DR is the second."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can iterate the above process several times to get more relibale results.\n",
    "\n",
    "Please see [../examples/opl](../opl) for a more sophisticated example of the evaluation of off-policy learners with synthetic bandit data."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.5 64-bit ('zr-obp': pyenv)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "interpreter": {
   "hash": "64b446a4e17784c2dc3dbe74bf0708929a003fb4822b30671d57ebdef413b716"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}