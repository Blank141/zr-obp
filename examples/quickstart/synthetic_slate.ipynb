{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quickstart Example with Synthetic Slate Bandit Data\n",
    "---\n",
    "This notebook provides an example of conducting OPE of several different evaluation policies with synthetic slate bandit feedback data.\n",
    "\n",
    "Our example with synthetic bandit data contains the follwoing four major steps:\n",
    "- (1) Synthetic Slate Data Generation\n",
    "- (2) Defining Evaluation Policy\n",
    "- (3) Off-Policy Evaluation\n",
    "- (4) Evaluation of OPE Estimators\n",
    "\n",
    "The second step could be replaced by some Off-Policy Learning (OPL) step, but obp still does not implement any OPL module for slate bandit data. Implementing OPL for slate bandit data is our future work.\n",
    "\n",
    "Please see [../examples/synthetic_slate](../synthetic_slate) for a more sophisticated example of the evaluation of OPE with synthetic slate bandit data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import open bandit pipeline (obp)\n",
    "import obp\n",
    "\n",
    "from obp.ope import SlateStandardIPS, SlateIndependentIPS, SlateRewardInteractionIPS, SlateOffPolicyEvaluation\n",
    "from obp.dataset import (\n",
    "    logistic_reward_function,\n",
    "    SyntheticSlateBanditDataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4.0\n"
     ]
    }
   ],
   "source": [
    "# obp version\n",
    "print(obp.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1) Synthetic Slate Data Generation\n",
    "We prepare easy-to-use synthetic slate data generator: `SyntheticSlateBanditDataset` class in the dataset module.\n",
    "\n",
    "It takes the following arguments as inputs and generates a synthetic bandit dataset that can be used to evaluate the performance of decision making policies (obtained by `off-policy learning`) and OPE estimators.\n",
    "- length of a list of actions recommended in each slate. (`len_list`)\n",
    "- number of unique actions (`n_unique_actions`)\n",
    "- dimension of context vectors (`dim_context`)\n",
    "- reward type (`reward_type`)\n",
    "- reward structure (`reward_structure`)\n",
    "- click model (`click_model`)\n",
    "- base reward function (`base_reward_function`)\n",
    "- behavior policy (`behavior_policy_function`)\n",
    "\n",
    "We use a uniform random policy as a behavior policy here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# generate a synthetic bandit dataset with 10 actions\n",
    "# we use `logistic_reward_function` as the reward function and `linear_behavior_policy_logit` as the behavior policy.\n",
    "# one can define their own reward function and behavior policy such as nonlinear ones. \n",
    "\n",
    "n_unique_action=10\n",
    "len_list = 3\n",
    "dim_context = 2\n",
    "reward_type = \"binary\"\n",
    "reward_structure=\"cascade_additive\"\n",
    "click_model=None\n",
    "random_state=12345\n",
    "base_reward_function=logistic_reward_function\n",
    "\n",
    "# obtain  test sets of synthetic logged bandit feedback\n",
    "n_rounds_test = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[sample_action_and_obtain_pscore]: 100%|██████████| 10000/10000 [00:01<00:00, 6317.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8366\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# define Uniform Random Policy as a baseline evaluation policy\n",
    "random_behavior_dataset = SyntheticSlateBanditDataset(\n",
    "    n_unique_action=n_unique_action,\n",
    "    len_list=len_list,\n",
    "    dim_context=dim_context,\n",
    "    reward_type=reward_type,\n",
    "    reward_structure=reward_structure,\n",
    "    click_model=click_model,\n",
    "    random_state=random_state,\n",
    "    behavior_policy_function=None,  # set to uniform random\n",
    "    base_reward_function=base_reward_function,\n",
    ")\n",
    "\n",
    "# compute the factual action choice probabililties for the test set of the synthetic logged bandit feedback\n",
    "random_behavior_feedback = random_behavior_dataset.obtain_batch_bandit_feedback(\n",
    "    n_rounds=n_rounds_test,\n",
    "    return_pscore_item_position=True,\n",
    ")\n",
    "\n",
    "# print policy value\n",
    "random_policy_value = random_behavior_dataset.calc_on_policy_policy_value(\n",
    "    reward=random_behavior_feedback[\"reward\"],\n",
    "    slate_id=random_behavior_feedback[\"slate_id\"],\n",
    ")\n",
    "print(random_policy_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2) Evaluation Policy Definition (Off-Policy Learning)\n",
    " After generating synthetic data, we now define the evaluation policy as follows:\n",
    " \n",
    "1. Generate logit values of three valuation policies (`random`, `optimal`, and `anti-optimal`).\n",
    "  - A `optimal` policy is defined by a policy that samples actions using`3 * base_expected_reward`.\n",
    "  - An `anti-optimal` policy is defined by a policy that samples actions using the sign inversion of `-3 * base_expected_reward`.\n",
    "2. Obtain pscores of the evaluation policies by `obtain_pscore_given_evaluation_policy_logit` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_policy_logit_ = np.zeros((n_rounds_test, n_unique_action))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_expected_reward = random_behavior_dataset.base_reward_function(\n",
    "    context=random_behavior_feedback[\"context\"],\n",
    "    action_context=random_behavior_dataset.action_context,\n",
    "    random_state=random_behavior_dataset.random_state,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_policy_logit_ = base_expected_reward * 3\n",
    "anti_optimal_policy_logit_ = -3 * base_expected_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[obtain_pscore_given_evaluation_policy_logit]: 100%|██████████| 10000/10000 [00:09<00:00, 1054.20it/s]\n"
     ]
    }
   ],
   "source": [
    "random_policy_pscores = random_behavior_dataset.obtain_pscore_given_evaluation_policy_logit(\n",
    "    action=random_behavior_feedback[\"action\"],\n",
    "    evaluation_policy_logit_=random_policy_logit_\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[obtain_pscore_given_evaluation_policy_logit]: 100%|██████████| 10000/10000 [00:09<00:00, 1019.17it/s]\n"
     ]
    }
   ],
   "source": [
    "optimal_policy_pscores = random_behavior_dataset.obtain_pscore_given_evaluation_policy_logit(\n",
    "    action=random_behavior_feedback[\"action\"],\n",
    "    evaluation_policy_logit_=optimal_policy_logit_\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[obtain_pscore_given_evaluation_policy_logit]: 100%|██████████| 10000/10000 [00:09<00:00, 1003.37it/s]\n"
     ]
    }
   ],
   "source": [
    "anti_optimal_policy_pscores = random_behavior_dataset.obtain_pscore_given_evaluation_policy_logit(\n",
    "    action=random_behavior_feedback[\"action\"],\n",
    "    evaluation_policy_logit_=anti_optimal_policy_logit_\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (3) Off-Policy Evaluation (OPE)\n",
    "Our next step is OPE which attempts to estimate the performance of evaluation policies using the logged bandit feedback and OPE estimators.\n",
    "\n",
    "Here, we use the **SlateStandardIPS (SIPS)**, **SlateIndependentIPS (IIPS)**, and **SlateRewardInteractionIPS (RIPS)** estimators and visualize the OPE results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# estimate the policy value of the evaluation policies based on their action choice probabilities\n",
    "# it is possible to set multiple OPE estimators to the `ope_estimators` argument\n",
    "\n",
    "sips = SlateStandardIPS(len_list=len_list)\n",
    "iips = SlateIndependentIPS(len_list=len_list)\n",
    "rips = SlateRewardInteractionIPS(len_list=len_list)\n",
    "\n",
    "ope = SlateOffPolicyEvaluation(\n",
    "    bandit_feedback=random_behavior_feedback,\n",
    "    ope_estimators=[sips, iips, rips]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, estimated_interval_random = ope.summarize_off_policy_estimates(\n",
    "    evaluation_policy_pscore=random_policy_pscores[0],\n",
    "    evaluation_policy_pscore_item_position=random_policy_pscores[1],\n",
    "    evaluation_policy_pscore_cascade=random_policy_pscores[2],\n",
    "    alpha=0.05,\n",
    "    n_bootstrap_samples=1000,\n",
    "    random_state=random_behavior_dataset.random_state,\n",
    ")\n",
    "estimated_interval_random[\"policy_name\"] = \"random\"\n",
    "\n",
    "print(estimated_interval_random, '\\n')\n",
    "# visualize estimated policy values of Uniform Random by the three OPE estimators\n",
    "# and their 95% confidence intervals (estimated by nonparametric bootstrap method)\n",
    "ope.visualize_off_policy_estimates(\n",
    "    evaluation_policy_pscore=random_policy_pscores[0],\n",
    "    evaluation_policy_pscore_item_position=random_policy_pscores[1],\n",
    "    evaluation_policy_pscore_cascade=random_policy_pscores[2],\n",
    "    alpha=0.05,\n",
    "    n_bootstrap_samples=1000, # number of resampling performed in the bootstrap procedure\n",
    "    random_state=random_behavior_dataset.random_state,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, estimated_interval_optimal = ope.summarize_off_policy_estimates(\n",
    "    evaluation_policy_pscore=optimal_policy_pscores[0],\n",
    "    evaluation_policy_pscore_item_position=optimal_policy_pscores[1],\n",
    "    evaluation_policy_pscore_cascade=optimal_policy_pscores[2],\n",
    "    alpha=0.05,\n",
    "    n_bootstrap_samples=1000,\n",
    "    random_state=random_behavior_dataset.random_state,\n",
    ")\n",
    "\n",
    "estimated_interval_optimal[\"policy_name\"] = \"optimal\"\n",
    "\n",
    "print(estimated_interval_optimal, '\\n')\n",
    "# visualize estimated policy values of Optimal by the three OPE estimators\n",
    "# and their 95% confidence intervals (estimated by nonparametric bootstrap method)\n",
    "ope.visualize_off_policy_estimates(\n",
    "    evaluation_policy_pscore=optimal_policy_pscores[0],\n",
    "    evaluation_policy_pscore_item_position=optimal_policy_pscores[1],\n",
    "    evaluation_policy_pscore_cascade=optimal_policy_pscores[2],\n",
    "    alpha=0.05,\n",
    "    n_bootstrap_samples=1000, # number of resampling performed in the bootstrap procedure\n",
    "    random_state=random_behavior_dataset.random_state,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, estimated_interval_anti_optimal = ope.summarize_off_policy_estimates(\n",
    "    evaluation_policy_pscore=anti_optimal_policy_pscores[0],\n",
    "    evaluation_policy_pscore_item_position=anti_optimal_policy_pscores[1],\n",
    "    evaluation_policy_pscore_cascade=anti_optimal_policy_pscores[2],\n",
    "    alpha=0.05,\n",
    "    n_bootstrap_samples=1000,\n",
    "    random_state=random_behavior_dataset.random_state,\n",
    ")\n",
    "estimated_interval_anti_optimal[\"policy_name\"] = \"anti-optimal\"\n",
    "\n",
    "print(estimated_interval_anti_optimal, '\\n')\n",
    "# visualize estimated policy values of Anti-optimal by the three OPE estimators\n",
    "# and their 95% confidence intervals (estimated by nonparametric bootstrap method)\n",
    "ope.visualize_off_policy_estimates(\n",
    "    evaluation_policy_pscore=anti_optimal_policy_pscores[0],\n",
    "    evaluation_policy_pscore_item_position=anti_optimal_policy_pscores[1],\n",
    "    evaluation_policy_pscore_cascade=anti_optimal_policy_pscores[2],\n",
    "    alpha=0.05,\n",
    "    n_bootstrap_samples=1000, # number of resampling performed in the bootstrap procedure\n",
    "    random_state=random_behavior_dataset.random_state,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (4) Evaluation of OPE estimators\n",
    "Our final step is **the evaluation of OPE**, which evaluates and compares the estimation accuracy of OPE estimators.\n",
    "\n",
    "With synthetic slate data, we can calculate the policy value of the evaluation policies. \n",
    "Therefore, we can compare the policy values estimated by OPE estimators with the ground-turths to evaluate the accuracy of OPE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_random = random_behavior_dataset.calc_ground_truth_policy_value(\n",
    "    context=random_behavior_feedback[\"context\"],\n",
    "    evaluation_policy_logit_=random_policy_logit_\n",
    ")\n",
    "gt_random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_optimal = random_behavior_dataset.calc_ground_truth_policy_value(\n",
    "    context=random_behavior_feedback[\"context\"],\n",
    "    evaluation_policy_logit_=optimal_policy_logit_\n",
    ")\n",
    "gt_optimal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_anti_optimal = random_behavior_dataset.calc_ground_truth_policy_value(\n",
    "    context=random_behavior_feedback[\"context\"],\n",
    "    evaluation_policy_logit_=anti_optimal_policy_logit_\n",
    ")\n",
    "gt_anti_optimal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimated_interval_random[\"ground_truth\"] = gt_random\n",
    "estimated_interval_optimal[\"ground_truth\"] = gt_optimal\n",
    "estimated_interval_anti_optimal[\"ground_truth\"] = gt_anti_optimal\n",
    "\n",
    "estimated_intervals = pd.concat(\n",
    "    [\n",
    "        estimated_interval_random,\n",
    "        estimated_interval_optimal,\n",
    "        estimated_interval_anti_optimal\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimated_intervals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can confirm that the three OPE estimators return the same results when the behavior policy and the evaluation policy is the same, and the estimates are quite similar to the `random_policy_value` calcurated above.\n",
    "\n",
    "We can also observe that the performance of OPE estimators are as follows in this simulation: `IIPS > RIPS > SIPS`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the estimation performances of OPE estimators \n",
    "# by comparing the estimated policy values and its ground-truth.\n",
    "# `summarize_estimators_comparison` returns a pandas dataframe containing estimation performances of given estimators \n",
    "\n",
    "relative_ee_for_random_evaluation_policy = ope.summarize_estimators_comparison(\n",
    "    ground_truth_policy_value=gt_random,\n",
    "    evaluation_policy_pscore=random_policy_pscores[0],\n",
    "    evaluation_policy_pscore_item_position=random_policy_pscores[1],\n",
    "    evaluation_policy_pscore_cascade=random_policy_pscores[2],\n",
    ")\n",
    "relative_ee_for_random_evaluation_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the estimation performances of OPE estimators \n",
    "# by comparing the estimated policy values and its ground-truth.\n",
    "# `summarize_estimators_comparison` returns a pandas dataframe containing estimation performances of given estimators \n",
    "\n",
    "relative_ee_for_optimal_evaluation_policy = ope.summarize_estimators_comparison(\n",
    "    ground_truth_policy_value=gt_optimal,\n",
    "    evaluation_policy_pscore=optimal_policy_pscores[0],\n",
    "    evaluation_policy_pscore_item_position=optimal_policy_pscores[1],\n",
    "    evaluation_policy_pscore_cascade=optimal_policy_pscores[2],\n",
    ")\n",
    "relative_ee_for_optimal_evaluation_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the estimation performances of OPE estimators \n",
    "# by comparing the estimated policy values and its ground-truth.\n",
    "# `summarize_estimators_comparison` returns a pandas dataframe containing estimation performances of given estimators \n",
    "\n",
    "relative_ee_for_anti_optimal_evaluation_policy = ope.summarize_estimators_comparison(\n",
    "    ground_truth_policy_value=gt_anti_optimal,\n",
    "    evaluation_policy_pscore=anti_optimal_policy_pscores[0],\n",
    "    evaluation_policy_pscore_item_position=anti_optimal_policy_pscores[1],\n",
    "    evaluation_policy_pscore_cascade=anti_optimal_policy_pscores[2],\n",
    ")\n",
    "relative_ee_for_anti_optimal_evaluation_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variance of OPE estimators is as follows: `SIPS > RIPS > IIPS`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimated_intervals[\"errbar_length\"] = (\n",
    "    estimated_intervals.drop([\"mean\", \"policy_name\", \"ground_truth\"], axis=1).diff(axis=1).iloc[:, -1].abs()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.05\n",
    "plt.style.use(\"ggplot\")\n",
    "\n",
    "def errplot(x, y, yerr, **kwargs):\n",
    "    ax = plt.gca()\n",
    "    data = kwargs.pop(\"data\")\n",
    "    data.plot(x=x, y=y, yerr=yerr, kind=\"bar\", ax=ax, **kwargs)\n",
    "    ax.hlines(data[\"ground_truth\"].iloc[0], -1, len(x)+1)\n",
    "#     ax.set_xlabel(\"OPE estimator\")\n",
    "    \n",
    "g = sns.FacetGrid(\n",
    "    estimated_intervals.reset_index().rename(columns={\"index\": \"OPE estimator\", \"mean\": \"Policy value\"}),\n",
    "    col=\"policy_name\"\n",
    ")\n",
    "g.map_dataframe(errplot, \"OPE estimator\", \"Policy value\", \"errbar_length\")\n",
    "plt.ylim((1.7, 1.9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is surprising that `RIPS` estimator does not achieve the best performance even if the reward structure is not independent. If we run a simuration where the reward of each position depends heavily on those of other positions, `RIPS`estimator could achieve the best performance.\n",
    "\n",
    "Please see [../examples/synthetic_slate](../synthetic_slate) for a more sophisticated example of the evaluation of OPE with synthetic slate bandit data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
